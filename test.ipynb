{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from data import read_vocab, read_category, batch_iter, process_file, build_vocab\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    embedding_dim = 128\n",
    "    seq_length = 25\n",
    "    num_classes = 3\n",
    "    vocab_size = 6282\n",
    "    trainable = True\n",
    "\n",
    "    num_layers = 2\n",
    "    hidden_dim = 128\n",
    "    rnn='lstm'\n",
    "\n",
    "    dropout_keep_prob = 0.8\n",
    "    lr = 1e-3\n",
    "    batch_size = 128\n",
    "    num_epochs = 10\n",
    "\n",
    "    print_per_batch = 100\n",
    "    save_per_batch = 10\n",
    "\n",
    "\n",
    "class LSTMModel(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        self.input_x = tf.placeholder(tf.int32, shape=[None, self.config.seq_length], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.float32, shape=[None, self.config.num_classes], name='input_y')\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "        self.rnn()\n",
    "\n",
    "    def rnn(self):\n",
    "        def lstm_cell():\n",
    "            return tf.contrib.rnn.BasicLSTMCell(self.config.hidden_dim, state_is_tuple=True)\n",
    "\n",
    "        def gru_cell():\n",
    "            return tf.contrib.rnn.GRUCell(self.config.hidden_dim)\n",
    "\n",
    "        def dropout():\n",
    "            if self.config.rnn == 'lstm':\n",
    "                cell = lstm_cell()\n",
    "            else:\n",
    "                cell = gru_cell()\n",
    "            return tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob)\n",
    "\n",
    "        with tf.device('/cpu:0'):\n",
    "            embedding = tf.get_variable('embedding', [self.config.vocab_size, self.config.embedding_dim],\n",
    "                                        trainable=self.config.trainable)  # [vocab_size, dim]\n",
    "            self.embedding_inputs = tf.nn.embedding_lookup(embedding, self.input_x)  # [batch_size, maxlen, dim]\n",
    "\n",
    "        with tf.name_scope(\"rnn\"):\n",
    "            cells = [dropout() for _ in range(self.config.num_layers)]\n",
    "            rnn_cell = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "\n",
    "            self._outputs, self.state = tf.nn.dynamic_rnn(cell=rnn_cell, inputs=self.embedding_inputs, dtype=tf.float32)  # _outputs.shape=[batch_size, maxlen, dim]\n",
    "            self.last = self._outputs[:, -1, :]  # [batch_size, hidden_dim]\n",
    "\n",
    "        with tf.name_scope(\"score\"):\n",
    "            fc = tf.layers.dense(self.last, self.config.hidden_dim, name='fc1')  # [batch_size, hidden_dim]\n",
    "            fc = tf.contrib.layers.dropout(fc, self.keep_prob)\n",
    "            fc = tf.nn.relu(fc)\n",
    "\n",
    "            self.logits = tf.layers.dense(fc, self.config.num_classes, name='fc2')  # [batch_size, num_classes]\n",
    "            self.y_pred_cls = tf.argmax(tf.nn.softmax(self.logits), 1)\n",
    "\n",
    "        with tf.name_scope(\"optimize\"):\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "            self.optim = tf.train.AdamOptimizer(learning_rate=self.config.lr).minimize(self.loss)\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_pred = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_cls)\n",
    "            self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = './data/2019-07-19'\n",
    "train_dir = os.path.join(base_dir, 'train.txt')\n",
    "test_dir = os.path.join(base_dir, 'test.txt')\n",
    "val_dir = os.path.join(base_dir, 'val.txt')\n",
    "vocab_dir = os.path.join(base_dir, 'vocab.txt')\n",
    "\n",
    "words, word2id = read_vocab(vocab_dir)\n",
    "categories, cat2id = read_category()\n",
    "\n",
    "x_train, y_train = process_file(train_dir, word2id, cat2id, 25)\n",
    "x_input = x_train[:128]\n",
    "y_input = y_train[:128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "config.vocab_size = len(words)\n",
    "model = LSTMModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {\n",
    "    model.input_x: x_input,\n",
    "    model.input_y: y_input,\n",
    "    model.keep_prob: 1.0\n",
    "}\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    emb = sess.run(model.embedding_inputs, feed_dict=feed_dict)\n",
    "    out = sess.run(model._outputs, feed_dict=feed_dict)\n",
    "    state = sess.run(model.state, feed_dict=feed_dict)\n",
    "    la = sess.run(model.last, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = out[:,-1,:]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state[1].h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
